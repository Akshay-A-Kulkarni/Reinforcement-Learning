{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q* Learning with OpenAI Taxi-v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4U320cILJc9QavlHYSYtu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshay-A-Kulkarni/Reinforcement-Learning/blob/master/Q_Learning_with_OpenAI_Taxi_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNwY_wudA-SC",
        "colab_type": "text"
      },
      "source": [
        "## Step 0: Import the dependencies \n",
        "First, we need to import the libraries <b>that we need to create our agent.</b></br>\n",
        "We use 3 libraries:\n",
        "- `Numpy` for our Qtable\n",
        "- `OpenAI Gym` for our Taxi Environment\n",
        "- `Random` to generate random numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2FypzV_A7WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swBQlxAQBIUL",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Create the environment \n",
        "- Here we'll create the Taxi environment. \n",
        "- OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqHB5SEBBEIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "946c2db8-65a0-40fe-8610-91d18e0558b7"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "env.render()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RF9wrmdGj2F",
        "colab_type": "text"
      },
      "source": [
        "### Random/ Brute Force Animation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fhz1BOSGhat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1cd30f51-84af-4aab-a4ef-fafd37ef8529"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "# env.s = 328  # set environment to illustration's state\n",
        "\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "done = False\n",
        "\n",
        "env.reset()\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "    \n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timesteps taken: 200\n",
            "Penalties incurred: 56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XldVYtHGU3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6353b8f6-30f1-49ae-cef1-51491822b9d1"
      },
      "source": [
        "def animate_frames(frames, wait_time):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(wait_time)\n",
        "        \n",
        "animate_frames(frames, 0.05)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "Timestep: 200\n",
            "State: 188\n",
            "Action: 2\n",
            "Reward: -1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWErrsNUB8D0",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Create the Q-table and initialize it üóÑÔ∏è\n",
        "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
        "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i7wq43jB0Ot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "00cb61b9-0c6c-4ba4-873e-b4f251ea77de"
      },
      "source": [
        "action_size = env.action_space.n\n",
        "print(\"Action size \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n\n",
        "print(\"State size \", state_size)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action size  6\n",
            "State size  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gQ2lYB8IV5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7f9a4cb7-02ff-4878-e2ac-3e78c3f4d711"
      },
      "source": [
        "Qtable = np.zeros((state_size,action_size))\n",
        "Qtable"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CewPo-IjIs1g",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Create the hyperparameters\n",
        "Here, we'll specify the hyperparameters to control our Q learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHIBO55BItWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Episode Params\n",
        "total_episodes = 50000        \n",
        "total_test_episodes = 100      \n",
        "max_steps = 100 \n",
        "\n",
        "\n",
        "# Learning & Discount(gamma) Rates\n",
        "lr = 0.7\n",
        "gamma = 0.618\n",
        "\n",
        "# Exploration Parameter \n",
        "epsilon = 1.0       # Exploration rate\n",
        "max_eps = 1.0\n",
        "min_eps = 0.01\n",
        "decay_rate = 0.01   # Exponential decay rate for exploration prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hMkbNYjJvip",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: The Q learning algorithm üß†\n",
        "- Now we implement the Q learning algorithm:\n",
        "\n",
        "\n",
        "<img src=\"https://cdn-media-1.freecodecamp.org/images/1*jmcVWHHbzCxDc-irBy9JTw.png\" alt=\"Q algo\" width = \"95%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wjEFb5TJrsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Begin Learning until convegence ie. until iteration limit of total episodes.\n",
        "for episode in range(total_episodes):\n",
        "  # reset the world\n",
        "  state = env.reset() \n",
        "  step = 0 \n",
        "  done = False\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    # Choose an Action in the current world state (s)\n",
        "\n",
        "    # First randomize a number\n",
        "    exp_exp_tradeoff = random.uniform(0,1)\n",
        "\n",
        "    #if this number is greater thatn our exploration paramter epsilon \n",
        "    #take action with the biggest Q value for this state\n",
        "    if exp_exp_tradeoff > epsilon:\n",
        "      action = np.argmax(Qtable[state,:])\n",
        "\n",
        "    # Otherwise take a random choice and explore\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "    \n",
        "    # Given action, observe the outcome state [s'] and reward [r]\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # Update using the Q-learning algo formula\n",
        "    # Q(s,a) := Q(s,a) + lr * [R(s,a) + gamma* max Q'(s',a') - Q(s,a)]\n",
        "\n",
        "    Qtable[state, action] = Qtable[state, action] + \\\n",
        "                              lr *(reward + gamma * np.max(Qtable[new_state,:]) \n",
        "                                - Qtable[state, action])\n",
        "                             \n",
        "    \n",
        "    # Set new state as current State\n",
        "    state = new_state\n",
        "\n",
        "    # If done i.e destination reached then finish episode.\n",
        "    if done == True:\n",
        "      break\n",
        "  \n",
        "  # Reduce epsilon exploration as we update and table \n",
        "  epsilon = min_eps + (max_eps - min_eps)*np.exp(-decay_rate*episode)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3KmpSSOR2YB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "a3b5f2d7-e8dd-4504-abb2-9482b195d6f2"
      },
      "source": [
        "# This is our learned Q-table.\n",
        "display(Qtable)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ -2.50421537,  -2.43487347,  -2.50427491,  -2.43480242,\n",
              "         -2.32039715, -11.43371452],\n",
              "       [ -1.84041247,  -1.35777503,  -1.84267184,  -1.35777017,\n",
              "         -0.57891593, -10.35777131],\n",
              "       ...,\n",
              "       [ -2.21309646,   0.68136372,  -2.25741178,  -2.154706  ,\n",
              "         -7.        , -10.50803626],\n",
              "       [ -2.25741178,  -2.13752647,  -2.32131447,  -2.22861473,\n",
              "         -9.1       ,  -7.        ],\n",
              "       [ -1.476412  ,  -1.5036658 ,  -1.21282   ,  11.36      ,\n",
              "        -10.477222  , -10.123666  ]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHMnbKnFKTbr",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Use our Q-table to play Taxi  üöñ\n",
        "- After 50 000 episodes, our Q-table can be used as a \"cheatsheet\" or guide to play Taxi.\n",
        "- By running this cell you can see our agent playing Taxi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJkQ0CnpJyKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "09e468de-8e5f-484f-d6ac-49db7c4b027d"
      },
      "source": [
        "env.reset()\n",
        "rewards = []\n",
        "test_frames = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  total_rewards = 0\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    test_frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "    # Take the action (index) that have the max expected future R given that S.\n",
        "    a = np.argmax(Qtable[state,:])\n",
        "\n",
        "    n_state, r, done, info = env.step(a)\n",
        "\n",
        "    total_rewards += r\n",
        "\n",
        "    if done:\n",
        "      rewards.append(total_rewards)\n",
        "      break\n",
        "    state = n_state\n",
        "\n",
        "env.close()\n",
        "print(\"Score over time: \" + str(sum(rewards)/total_test_episodes))\n",
        "\n",
        "animate_frames(test_frames, wait_time = 0.005)\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "Timestep: 1289\n",
            "State: 16\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0agx1e1SEUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}